{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vajir/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/vajir/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/vajir/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/vajir/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data set\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: int64(785)\n",
      "memory usage: 251.5 MB\n"
     ]
    }
   ],
   "source": [
    "# train data info\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (42000, 784)\n",
      "Y_train shape = (42000,)\n",
      "X_test shape = (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_data.iloc[: ,1:].values\n",
    "Y_train = train_data.iloc[: ,0].values\n",
    "X_test = test_data.iloc[: ,:].values\n",
    "print('X_train shape =' ,X_train.shape)\n",
    "print('Y_train shape =' ,Y_train.shape)\n",
    "print('X_test shape =' ,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vajir/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/vajir/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/vajir/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33600, 784)\n",
      "(8400, 784)\n",
      "(33600,)\n",
      "(8400,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train_std ,X_cv_std ,Y_train_std ,Y_cv_std = train_test_split(X_train ,Y_train ,test_size =0.2 ,random_state =0)\n",
    "print(X_train_std.shape)\n",
    "print(X_cv_std.shape)\n",
    "print(Y_train_std.shape)\n",
    "print(Y_cv_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 33600)\n",
      "(10, 33600)\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing  \n",
    "X_train_std = (np.transpose(X_train_std))/255.0\n",
    "X_cv_std = (np.transpose(X_cv_std))/255.0\n",
    "X_test_std = (np.transpose(X_test))/255.0\n",
    "Y_train_std = np.transpose(to_categorical(Y_train_std ,num_classes =10))\n",
    "Y_cv_std = np.transpose(to_categorical(Y_cv_std ,num_classes =10))\n",
    "print(X_train_std.shape)\n",
    "print(Y_train_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating placeholder \n",
    "def create_placeholder(n_x ,n_y):\n",
    "    X = tf.placeholder(tf.float32 ,shape=[n_x ,None] )\n",
    "    Y = tf.placeholder(tf.float32 ,shape =[n_y ,None] )\n",
    "    return X ,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# network architecture\n",
    "tf.reset_default_graph()\n",
    "def initialize_parameter():\n",
    "    layer = np.array((784 ,500,100 ,10))\n",
    "    parameters ={}\n",
    "    for i in range(0 ,len(layer)-1):\n",
    "        parameters['W' + str(i+1)] = tf.get_variable(('W' +str(i+1)) ,[layer[i+1], layer[i]] ,initializer= tf.contrib.layers.xavier_initializer())\n",
    "        parameters['b' + str(i+1)] = tf.get_variable('b' +str(i+1) ,[layer[i+1] ,1] ,initializer = tf.zeros_initializer())\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forward loop\n",
    "def forward_prop(X ,parameters ,layer):\n",
    "    a_prev =X\n",
    "    linear_cache ={}\n",
    "    activation_cache ={}\n",
    "    for i in range(0 ,len(layer)-1):\n",
    "        linear_cache['Z' +str(i+1)] = tf.add(tf.matmul(parameters['W' +str(i+1)] ,a_prev) ,parameters['b' +str(i+1)])\n",
    "        activation_cache['A' + str(i+1)] = tf.nn.relu(linear_cache['Z' +str(i+1)])\n",
    "        a_prev = activation_cache['A' +str(i+1)]\n",
    "    return linear_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# computing cost  Y -(num_classes ,m)\n",
    "def compute_cost(X ,parameters ,layer ,Y):\n",
    "    linear_cache = forward_prop(X ,parameters ,layer)\n",
    "    logits = tf.transpose(linear_cache['Z' +str(len(layer) -1)])\n",
    "    labels = tf.transpose(Y)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits =logits ,labels = labels))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "def compute_cost_with_regularization(X,parameters,layer ,Y ,lambd ,m):\n",
    "    entropy_cost = compute_cost(X ,parameters ,layer ,Y)\n",
    "    entropy_cost = tf.cast(entropy_cost, tf.float32)\n",
    "    L2_cost = 0.0\n",
    "    for i in range(1 ,len(layer)):\n",
    "        L2_cost = L2_cost + tf.reduce_sum(tf.square(parameters['W' + str(i)]))\n",
    "    L2_cost = tf.multiply((tf.cast(L2_cost, tf.float32)) , lambd/(2.0*m))\n",
    "    cost = tf.cast(entropy_cost + L2_cost ,tf.float32)\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "def model(X_train_std ,Y_train_std,learning_rate =0.01,num_epoch =11480 ,lambd =0.6):\n",
    "    n_x = 784 \n",
    "    n_y = 10\n",
    "    m = X_train_std.shape[1]\n",
    "    costs =[]\n",
    "    layer = np.array((784 ,500,100 ,10))\n",
    "    X,Y = create_placeholder(n_x ,n_y)\n",
    "    parameters =initialize_parameter()\n",
    "    linear_cache = forward_prop(X ,parameters ,layer)\n",
    "    cost = compute_cost_with_regularization(X ,parameters ,layer ,Y ,lambd ,m)\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate =learning_rate).minimize(cost)\n",
    "    init = tf.global_variables_initializer()      # intitalizing the global variables\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epoch):\n",
    "        \n",
    "        _ ,epoch_cost = sess.run([optimizer ,cost] ,feed_dict ={X:X_train_std ,Y:Y_train_std})\n",
    "        if epoch% 5 == 0 :\n",
    "            print('cost after %d epoch is %f '%(epoch ,epoch_cost))\n",
    "        costs.append(epoch_cost)\n",
    "    plt.plot(np.squeeze(costs))         #  plot graph between cost and num of iteration\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    parameters = sess.run(parameters)      # saving parameters\n",
    "    sess.close()\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "parameters = model(X_train_std ,Y_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(X_train_std ,Y_train_std , parameters):\n",
    "    n_x =784\n",
    "    n_y = 10\n",
    "    layer = np.array((784 ,500,100 ,10))\n",
    "    X ,Y = create_placeholder(n_x ,n_y)\n",
    "    cache = forward_prop(X ,parameters ,layer)\n",
    "    sess= tf.Session()\n",
    "    linear_cache = sess.run(cache ,feed_dict ={X:X_train_std})\n",
    "    Y_pred = linear_cache['Z' +str(len(layer)-1)]\n",
    "    correct_prediction = tf.equal(tf.argmax(Y_pred), tf.argmax(Y_train_std))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction ,'float'))\n",
    "    acc = sess.run(accuracy)\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:0.98854166\n"
     ]
    }
   ],
   "source": [
    "train_acc = accuracy(X_train_std ,Y_train_std ,parameters)\n",
    "print('train accuracy:' + str(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation accuracy:0.9697619\n"
     ]
    }
   ],
   "source": [
    "cross_val_acc = accuracy(X_cv_std ,Y_cv_std ,parameters)\n",
    "print('cross validation accuracy:' + str(cross_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X_test_std ,parameters):\n",
    "    n_x =784\n",
    "    n_y = 10\n",
    "    layer = np.array((784 ,500,100 ,10))\n",
    "    X ,Y = create_placeholder(n_x ,n_y)\n",
    "    sess= tf.Session()\n",
    "    cache = forward_prop(X ,parameters ,layer)\n",
    "    linear_cache = sess.run(cache ,feed_dict ={X:X_test_std})\n",
    "    Y_pred = linear_cache['Z' +str(len(layer)-1)]\n",
    "    Y_test = np.argmax(Y_pred ,axis =0)\n",
    "    sess.close()\n",
    "    return Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_pred = predict(X_test_std ,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('sample_submission.csv')\n",
    "Y1 = data1.iloc[: ,0].values\n",
    "Y = {'ImageId':Y1 ,'Label':Y_test_pred}\n",
    "df = pd.DataFrame(data =Y ,dtype =int)\n",
    "df.to_csv('Data_recog_submission.csv' ,sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
